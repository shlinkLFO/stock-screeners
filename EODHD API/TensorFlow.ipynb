{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2025-04-11', '2025-04-10', '2025-04-09', '2025-04-08',\n",
       "       '2025-04-07', '2025-04-04', '2025-04-03', '2025-04-02',\n",
       "       '2025-04-01', '2025-03-31', '2025-03-28', '2025-03-27',\n",
       "       '2025-03-26', '2025-03-25', '2025-03-24', '2025-03-21',\n",
       "       '2025-03-20', '2025-03-19', '2025-03-18', '2025-03-17',\n",
       "       '2025-03-14'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('qqq_merged_data_engineered.csv')\n",
    "\n",
    "df['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping columns with NA values: (3921, 50)\n",
      "Columns with NA values: ['previous_date', 'daily_return', 'sma_5', 'sma_10', 'rsi_14', 'atr_14']\n",
      "Shape after dropping columns with NA values: (3921, 44)\n",
      "\n",
      "Remaining columns:\n",
      "['Date', 'Close/Last', 'ohlcv_volume', 'Open', 'High', 'Low', 'strike', 'open', 'high', 'low', 'last', 'last_size', 'change', 'pctchange', 'previous', 'bid', 'bid_size', 'ask', 'ask_size', 'moneyness', 'option_volume', 'volume_change', 'volume_pctchange', 'open_interest', 'open_interest_change', 'open_interest_pctchange', 'volatility', 'volatility_change', 'volatility_pctchange', 'theoretical', 'delta', 'gamma', 'theta', 'vega', 'rho', 'vol_oi_ratio', 'midpoint', 'put', 'ema_12', 'ema_26', 'macd', 'macd_signal', 'intraday_range_pct', 'options_to_ohlcv_volume_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with NA values\n",
    "print(\"Shape before dropping columns with NA values:\", df.shape)\n",
    "\n",
    "# Get columns with NA values\n",
    "columns_with_na = df.columns[df.isna().any()].tolist()\n",
    "print(f\"Columns with NA values: {columns_with_na}\")\n",
    "\n",
    "# Drop columns with NA values\n",
    "df = df.dropna(axis=1)\n",
    "\n",
    "print(\"Shape after dropping columns with NA values:\", df.shape)\n",
    "\n",
    "# Display remaining columns\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA Values Count per Column:\n",
      "Date: 0\n",
      "Close/Last: 0\n",
      "ohlcv_volume: 0\n",
      "Open: 0\n",
      "High: 0\n",
      "Low: 0\n",
      "strike: 0\n",
      "open: 0\n",
      "high: 0\n",
      "low: 0\n",
      "last: 0\n",
      "last_size: 0\n",
      "change: 0\n",
      "pctchange: 0\n",
      "previous: 0\n",
      "bid: 0\n",
      "bid_size: 0\n",
      "ask: 0\n",
      "ask_size: 0\n",
      "moneyness: 0\n",
      "option_volume: 0\n",
      "volume_change: 0\n",
      "volume_pctchange: 0\n",
      "open_interest: 0\n",
      "open_interest_change: 0\n",
      "open_interest_pctchange: 0\n",
      "volatility: 0\n",
      "volatility_change: 0\n",
      "volatility_pctchange: 0\n",
      "theoretical: 0\n",
      "delta: 0\n",
      "gamma: 0\n",
      "theta: 0\n",
      "vega: 0\n",
      "rho: 0\n",
      "vol_oi_ratio: 0\n",
      "midpoint: 0\n",
      "put: 0\n",
      "ema_12: 0\n",
      "ema_26: 0\n",
      "macd: 0\n",
      "macd_signal: 0\n",
      "intraday_range_pct: 0\n",
      "options_to_ohlcv_volume_ratio: 0\n",
      "\n",
      "Percentage of NA Values per Column:\n",
      "Date: 0.00%\n",
      "Close/Last: 0.00%\n",
      "ohlcv_volume: 0.00%\n",
      "Open: 0.00%\n",
      "High: 0.00%\n",
      "Low: 0.00%\n",
      "strike: 0.00%\n",
      "open: 0.00%\n",
      "high: 0.00%\n",
      "low: 0.00%\n",
      "last: 0.00%\n",
      "last_size: 0.00%\n",
      "change: 0.00%\n",
      "pctchange: 0.00%\n",
      "previous: 0.00%\n",
      "bid: 0.00%\n",
      "bid_size: 0.00%\n",
      "ask: 0.00%\n",
      "ask_size: 0.00%\n",
      "moneyness: 0.00%\n",
      "option_volume: 0.00%\n",
      "volume_change: 0.00%\n",
      "volume_pctchange: 0.00%\n",
      "open_interest: 0.00%\n",
      "open_interest_change: 0.00%\n",
      "open_interest_pctchange: 0.00%\n",
      "volatility: 0.00%\n",
      "volatility_change: 0.00%\n",
      "volatility_pctchange: 0.00%\n",
      "theoretical: 0.00%\n",
      "delta: 0.00%\n",
      "gamma: 0.00%\n",
      "theta: 0.00%\n",
      "vega: 0.00%\n",
      "rho: 0.00%\n",
      "vol_oi_ratio: 0.00%\n",
      "midpoint: 0.00%\n",
      "put: 0.00%\n",
      "ema_12: 0.00%\n",
      "ema_26: 0.00%\n",
      "macd: 0.00%\n",
      "macd_signal: 0.00%\n",
      "intraday_range_pct: 0.00%\n",
      "options_to_ohlcv_volume_ratio: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Count NA values for each column in the dataframe\n",
    "na_counts = df.isna().sum()\n",
    "\n",
    "# Display the count of NA values for each column\n",
    "print(\"NA Values Count per Column:\")\n",
    "for column, count in na_counts.items():\n",
    "    print(f\"{column}: {count}\")\n",
    "\n",
    "# Calculate percentage of NA values\n",
    "total_rows = len(df)\n",
    "na_percentage = (na_counts / total_rows) * 100\n",
    "\n",
    "# Display percentage of NA values\n",
    "print(\"\\nPercentage of NA Values per Column:\")\n",
    "for column, percentage in na_percentage.items():\n",
    "    print(f\"{column}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique dates in cleaned data: ['2025-04-11' '2025-04-10' '2025-04-09' '2025-04-08' '2025-04-07'\n",
      " '2025-04-04' '2025-04-03' '2025-04-02' '2025-04-01' '2025-03-31'\n",
      " '2025-03-28' '2025-03-27' '2025-03-26' '2025-03-25' '2025-03-24'\n",
      " '2025-03-21' '2025-03-20' '2025-03-19' '2025-03-18' '2025-03-17'\n",
      " '2025-03-14']\n",
      "Total number of unique dates: 21\n"
     ]
    }
   ],
   "source": [
    "unique_dates = df['Date'].unique()\n",
    "print(\"Unique dates in cleaned data:\", unique_dates)\n",
    "print(\"Total number of unique dates:\", len(unique_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (15, 5, 347, 43)\n",
      "y shape: (15,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"advanced_options_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"advanced_options_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ seq_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_7              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ seq_input (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m347\u001b[0m, \u001b[38;5;34m43\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_7              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m2,816\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m23,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,867</span> (101.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,867\u001b[0m (101.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,867</span> (101.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,867\u001b[0m (101.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 26s/step - loss: 1.3032 - val_loss: 0.2980\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.2364 - val_loss: 0.2104\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.1733 - val_loss: 0.1641\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.1119 - val_loss: 0.1644\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.0511 - val_loss: 0.2133\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.9903 - val_loss: 0.3110\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.9294 - val_loss: 0.4564\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.8678 - val_loss: 0.6490\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.8049 - val_loss: 0.8913\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.7410 - val_loss: 1.1856\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6761 - val_loss: 1.5377\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.6107 - val_loss: 1.9571\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5459 - val_loss: 2.4535\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4830 - val_loss: 3.0342\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4238 - val_loss: 3.7006\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3710 - val_loss: 4.4405\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3277 - val_loss: 5.2340\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2969 - val_loss: 6.0671\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2812 - val_loss: 6.9224\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2807 - val_loss: 7.7562\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  True_Close_next  Predicted_Close_next\n",
      "0  2025-03-21       490.660004            497.987366\n",
      "1  2025-03-24       493.459991            497.142334\n",
      "2  2025-03-25       484.380005            491.547577\n",
      "3  2025-03-26       481.619995            489.030579\n",
      "4  2025-03-27       468.940002            478.375122\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, GlobalAveragePooling1D, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Load and Prepare the Data\n",
    "# -----------------------------------\n",
    "\n",
    "# Sort the dataframe by Date (oldest first)\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Convert 'Date' column to datetime type\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "\n",
    "# Define the list of feature columns. Here we include every column except \"Date\".\n",
    "# (The \"Close/Last\" column will be used as a feature for the current day, but the target is the next day's value.)\n",
    "feature_cols = ['Close/Last', 'ohlcv_volume', 'Open', 'High', 'Low', 'strike', 'open', 'high', 'low', 'last', 'last_size', 'change', 'pctchange', 'previous', 'bid', 'bid_size', 'ask', 'ask_size', 'moneyness', 'option_volume', 'volume_change', 'volume_pctchange', 'open_interest', 'open_interest_change', 'open_interest_pctchange', 'volatility', 'volatility_change', 'volatility_pctchange', 'theoretical', 'delta', 'gamma', 'theta', 'vega', 'rho', 'vol_oi_ratio', 'midpoint', 'put', 'ema_12', 'ema_26', 'macd', 'macd_signal', 'intraday_range_pct', 'options_to_ohlcv_volume_ratio']\n",
    "\n",
    "# Group by each unique day so that each day’s options data is retained\n",
    "grouped = df.groupby('Date')\n",
    "\n",
    "# Create lists to hold each day's options data and corresponding date\n",
    "days = []\n",
    "dates = []\n",
    "for date, group in grouped:\n",
    "    # Extract the feature values from the group\n",
    "    day_data = group[feature_cols].values.astype(np.float32)\n",
    "    days.append(day_data)\n",
    "    dates.append(date)\n",
    "\n",
    "# Sort the days by date\n",
    "sorted_indices = np.argsort(dates)\n",
    "days = [days[i] for i in sorted_indices]\n",
    "dates = [dates[i] for i in sorted_indices]\n",
    "\n",
    "# Create target values:\n",
    "# For each day, we want to predict the next day’s close.\n",
    "# We assume that the day's \"Close/Last\" is the same for all rows of that day,\n",
    "# so we take the first row's \"Close/Last\" from the next day.\n",
    "targets = []\n",
    "for i in range(len(days) - 1):\n",
    "    next_day_close = days[i+1][0, feature_cols.index('Close/Last')]\n",
    "    targets.append(next_day_close)\n",
    "    \n",
    "# Remove the last day as it has no following day's close to predict\n",
    "days = days[:-1]\n",
    "dates = dates[:-1]\n",
    "targets = np.array(targets)\n",
    "\n",
    "# Since each day can have a variable number of options records,\n",
    "# we pad the data so that every day has the same shape.\n",
    "max_options = max(day.shape[0] for day in days)\n",
    "# days_padded will have shape (n_days, max_options, n_features)\n",
    "days_padded = pad_sequences(days, maxlen=max_options, dtype='float32', \n",
    "                            padding='post', truncating='post')\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Create Lookback Sequences\n",
    "# -----------------------------------\n",
    "# Use a lookback window so that each training sample is\n",
    "# built from the previous \"lookback\" days' options data.\n",
    "lookback = 5  # Example: use the previous 5 days\n",
    "\n",
    "X, y, X_dates = [], [], []\n",
    "for i in range(lookback, len(targets)):\n",
    "    # X: sequence of day data from the past \"lookback\" days\n",
    "    X.append(days_padded[i-lookback:i])\n",
    "    # y: the target for the current day (next day close)\n",
    "    y.append(targets[i])\n",
    "    X_dates.append(dates[i])\n",
    "    \n",
    "X = np.array(X)  # shape: (n_samples, lookback, max_options, n_features)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Scale the Data\n",
    "# -----------------------------------\n",
    "# Scale X: reshape to 2D, scale, then reshape back.\n",
    "n_samples, lb, max_opts, n_features = X.shape\n",
    "X_reshaped = X.reshape(-1, n_features)\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled_reshaped = scaler_X.fit_transform(X_reshaped)\n",
    "X_scaled = X_scaled_reshaped.reshape(n_samples, lb, max_opts, n_features)\n",
    "\n",
    "# Scale y:\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Build the LSTM RNN Model with L2 Regularization\n",
    "# -----------------------------------\n",
    "# Use L2 regularization factor (adjust as needed)\n",
    "l2_reg = 0.001\n",
    "\n",
    "# Number of features remains the same.\n",
    "n_features = len(feature_cols)\n",
    "\n",
    "# Sub-model to process a single day’s options data.\n",
    "# Input shape: (max_options, n_features)\n",
    "option_input = Input(shape=(max_options, n_features), name='option_input')\n",
    "# Mask padded rows (assumed to be zeros)\n",
    "masked = Masking(mask_value=0.0)(option_input)\n",
    "# Process each option row individually with a TimeDistributed Dense layer including L2 regularization.\n",
    "option_dense = TimeDistributed(Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)))(masked)\n",
    "# Aggregate the processed rows into a fixed-length vector using global average pooling.\n",
    "day_embedding = GlobalAveragePooling1D()(option_dense)\n",
    "# Create the day-level model that outputs a daily embedding.\n",
    "day_model = Model(inputs=option_input, outputs=day_embedding, name='day_model')\n",
    "\n",
    "# Define the sequence model.\n",
    "# Input: sequence of days with shape (lookback, max_options, n_features)\n",
    "seq_input = Input(shape=(lookback, max_options, n_features), name='seq_input')\n",
    "# Apply the day_model to each day in the sequence using TimeDistributed.\n",
    "day_embeddings = TimeDistributed(day_model)(seq_input)  # shape: (lookback, 64)\n",
    "# Feed the sequence of day embeddings into an LSTM layer to capture temporal dynamics.\n",
    "lstm_out = LSTM(50, activation='tanh')(day_embeddings)\n",
    "# Final Dense layer to predict the next day’s closing price with L2 regularization.\n",
    "output = Dense(1, kernel_regularizer=l2(l2_reg))(lstm_out)\n",
    "\n",
    "# Build and compile the complete model.\n",
    "model = Model(inputs=seq_input, outputs=output, name='advanced_options_model')\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Train the Model\n",
    "# -----------------------------------\n",
    "# Train using the scaled data.\n",
    "history = model.fit(X_scaled, y_scaled, epochs=20, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# -----------------------------------\n",
    "# 6. Evaluate and Save Predictions\n",
    "# -----------------------------------\n",
    "# Generate predictions on the entire dataset.\n",
    "predictions_scaled = model.predict(X_scaled)\n",
    "# Invert target scaling\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': [d.strftime('%Y-%m-%d') for d in X_dates],\n",
    "    'True_Close_next': y.flatten(),\n",
    "    'Predicted_Close_next': predictions.flatten()\n",
    "})\n",
    "print(results_df.head())\n",
    "\n",
    "# Save the trained model and the predictions for later analysis.\n",
    "model.save('advanced_options_close_predictor.h5')\n",
    "results_df.to_csv('advanced_expanding_window_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
