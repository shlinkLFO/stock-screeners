{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2025-04-11', '2025-04-10', '2025-04-09', '2025-04-08',\n",
       "       '2025-04-07', '2025-04-04', '2025-04-03', '2025-04-02',\n",
       "       '2025-04-01', '2025-03-31', '2025-03-28', '2025-03-27',\n",
       "       '2025-03-26', '2025-03-25', '2025-03-24', '2025-03-21',\n",
       "       '2025-03-20', '2025-03-19', '2025-03-18', '2025-03-17',\n",
       "       '2025-03-14'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('qqq_merged_data_engineered.csv')\n",
    "\n",
    "df['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping columns with NA values: (3921, 50)\n",
      "Columns with NA values: ['previous_date', 'daily_return', 'sma_5', 'sma_10', 'rsi_14', 'atr_14']\n",
      "Shape after dropping columns with NA values: (3921, 44)\n",
      "\n",
      "Remaining columns:\n",
      "['Date', 'Close/Last', 'ohlcv_volume', 'Open', 'High', 'Low', 'strike', 'open', 'high', 'low', 'last', 'last_size', 'change', 'pctchange', 'previous', 'bid', 'bid_size', 'ask', 'ask_size', 'moneyness', 'option_volume', 'volume_change', 'volume_pctchange', 'open_interest', 'open_interest_change', 'open_interest_pctchange', 'volatility', 'volatility_change', 'volatility_pctchange', 'theoretical', 'delta', 'gamma', 'theta', 'vega', 'rho', 'vol_oi_ratio', 'midpoint', 'put', 'ema_12', 'ema_26', 'macd', 'macd_signal', 'intraday_range_pct', 'options_to_ohlcv_volume_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with NA values\n",
    "print(\"Shape before dropping columns with NA values:\", df.shape)\n",
    "\n",
    "# Get columns with NA values\n",
    "columns_with_na = df.columns[df.isna().any()].tolist()\n",
    "print(f\"Columns with NA values: {columns_with_na}\")\n",
    "\n",
    "# Drop columns with NA values\n",
    "df = df.dropna(axis=1)\n",
    "\n",
    "print(\"Shape after dropping columns with NA values:\", df.shape)\n",
    "\n",
    "# Display remaining columns\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA Values Count per Column:\n",
      "Date: 0\n",
      "Close/Last: 0\n",
      "ohlcv_volume: 0\n",
      "Open: 0\n",
      "High: 0\n",
      "Low: 0\n",
      "strike: 0\n",
      "open: 0\n",
      "high: 0\n",
      "low: 0\n",
      "last: 0\n",
      "last_size: 0\n",
      "change: 0\n",
      "pctchange: 0\n",
      "previous: 0\n",
      "bid: 0\n",
      "bid_size: 0\n",
      "ask: 0\n",
      "ask_size: 0\n",
      "moneyness: 0\n",
      "option_volume: 0\n",
      "volume_change: 0\n",
      "volume_pctchange: 0\n",
      "open_interest: 0\n",
      "open_interest_change: 0\n",
      "open_interest_pctchange: 0\n",
      "volatility: 0\n",
      "volatility_change: 0\n",
      "volatility_pctchange: 0\n",
      "theoretical: 0\n",
      "delta: 0\n",
      "gamma: 0\n",
      "theta: 0\n",
      "vega: 0\n",
      "rho: 0\n",
      "vol_oi_ratio: 0\n",
      "midpoint: 0\n",
      "put: 0\n",
      "ema_12: 0\n",
      "ema_26: 0\n",
      "macd: 0\n",
      "macd_signal: 0\n",
      "intraday_range_pct: 0\n",
      "options_to_ohlcv_volume_ratio: 0\n",
      "\n",
      "Percentage of NA Values per Column:\n",
      "Date: 0.00%\n",
      "Close/Last: 0.00%\n",
      "ohlcv_volume: 0.00%\n",
      "Open: 0.00%\n",
      "High: 0.00%\n",
      "Low: 0.00%\n",
      "strike: 0.00%\n",
      "open: 0.00%\n",
      "high: 0.00%\n",
      "low: 0.00%\n",
      "last: 0.00%\n",
      "last_size: 0.00%\n",
      "change: 0.00%\n",
      "pctchange: 0.00%\n",
      "previous: 0.00%\n",
      "bid: 0.00%\n",
      "bid_size: 0.00%\n",
      "ask: 0.00%\n",
      "ask_size: 0.00%\n",
      "moneyness: 0.00%\n",
      "option_volume: 0.00%\n",
      "volume_change: 0.00%\n",
      "volume_pctchange: 0.00%\n",
      "open_interest: 0.00%\n",
      "open_interest_change: 0.00%\n",
      "open_interest_pctchange: 0.00%\n",
      "volatility: 0.00%\n",
      "volatility_change: 0.00%\n",
      "volatility_pctchange: 0.00%\n",
      "theoretical: 0.00%\n",
      "delta: 0.00%\n",
      "gamma: 0.00%\n",
      "theta: 0.00%\n",
      "vega: 0.00%\n",
      "rho: 0.00%\n",
      "vol_oi_ratio: 0.00%\n",
      "midpoint: 0.00%\n",
      "put: 0.00%\n",
      "ema_12: 0.00%\n",
      "ema_26: 0.00%\n",
      "macd: 0.00%\n",
      "macd_signal: 0.00%\n",
      "intraday_range_pct: 0.00%\n",
      "options_to_ohlcv_volume_ratio: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Count NA values for each column in the dataframe\n",
    "na_counts = df.isna().sum()\n",
    "\n",
    "# Display the count of NA values for each column\n",
    "print(\"NA Values Count per Column:\")\n",
    "for column, count in na_counts.items():\n",
    "    print(f\"{column}: {count}\")\n",
    "\n",
    "# Calculate percentage of NA values\n",
    "total_rows = len(df)\n",
    "na_percentage = (na_counts / total_rows) * 100\n",
    "\n",
    "# Display percentage of NA values\n",
    "print(\"\\nPercentage of NA Values per Column:\")\n",
    "for column, percentage in na_percentage.items():\n",
    "    print(f\"{column}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique dates in cleaned data: ['2025-04-11' '2025-04-10' '2025-04-09' '2025-04-08' '2025-04-07'\n",
      " '2025-04-04' '2025-04-03' '2025-04-02' '2025-04-01' '2025-03-31'\n",
      " '2025-03-28' '2025-03-27' '2025-03-26' '2025-03-25' '2025-03-24'\n",
      " '2025-03-21' '2025-03-20' '2025-03-19' '2025-03-18' '2025-03-17'\n",
      " '2025-03-14']\n",
      "Total number of unique dates: 21\n"
     ]
    }
   ],
   "source": [
    "unique_dates = df['Date'].unique()\n",
    "print(\"Unique dates in cleaned data:\", unique_dates)\n",
    "print(\"Total number of unique dates:\", len(unique_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (15, 5, 347, 43)\n",
      "y shape: (15,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"advanced_direction_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"advanced_direction_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ seq_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">347</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,816</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ seq_input (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m347\u001b[0m, \u001b[38;5;34m43\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │         \u001b[38;5;34m2,816\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m23,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,867</span> (101.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,867\u001b[0m (101.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,867</span> (101.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,867\u001b[0m (101.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25s/step - accuracy: 0.4167 - loss: 0.7979 - val_accuracy: 0.6667 - val_loss: 0.6981\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4167 - loss: 0.7731 - val_accuracy: 0.6667 - val_loss: 0.7213\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4167 - loss: 0.7549 - val_accuracy: 0.3333 - val_loss: 0.7500\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7427 - val_accuracy: 0.3333 - val_loss: 0.7827\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7357 - val_accuracy: 0.3333 - val_loss: 0.8177\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5833 - loss: 0.7327 - val_accuracy: 0.3333 - val_loss: 0.8523\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7326 - val_accuracy: 0.3333 - val_loss: 0.8835\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7337 - val_accuracy: 0.3333 - val_loss: 0.9088\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5833 - loss: 0.7347 - val_accuracy: 0.3333 - val_loss: 0.9269\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5833 - loss: 0.7349 - val_accuracy: 0.3333 - val_loss: 0.9380\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5833 - loss: 0.7340 - val_accuracy: 0.3333 - val_loss: 0.9433\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7322 - val_accuracy: 0.3333 - val_loss: 0.9439\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7300 - val_accuracy: 0.3333 - val_loss: 0.9413\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7275 - val_accuracy: 0.3333 - val_loss: 0.9366\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7251 - val_accuracy: 0.3333 - val_loss: 0.9309\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5833 - loss: 0.7230 - val_accuracy: 0.3333 - val_loss: 0.9251\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7212 - val_accuracy: 0.3333 - val_loss: 0.9198\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5833 - loss: 0.7198 - val_accuracy: 0.3333 - val_loss: 0.9155\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5833 - loss: 0.7187 - val_accuracy: 0.3333 - val_loss: 0.9128\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5833 - loss: 0.7178 - val_accuracy: 0.3333 - val_loss: 0.9117\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  True_Direction  Predicted_Direction\n",
      "0  2025-03-21               1                    0\n",
      "1  2025-03-24               1                    0\n",
      "2  2025-03-25               0                    0\n",
      "3  2025-03-26               0                    0\n",
      "4  2025-03-27               0                    0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, GlobalAveragePooling1D, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Load and Prepare the Data\n",
    "# -----------------------------------\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Sort the dataframe by Date (oldest first)\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Define the list of feature columns. Here we include every column except \"Date\".\n",
    "# (The \"Close/Last\" column will be used to derive the target.)\n",
    "feature_cols = ['Close/Last', 'ohlcv_volume', 'Open', 'High', 'Low', 'strike', 'open', 'high', 'low', 'last', 'last_size', 'change', 'pctchange', 'previous', 'bid', 'bid_size', 'ask', 'ask_size', 'moneyness', 'option_volume', 'volume_change', 'volume_pctchange', 'open_interest', 'open_interest_change', 'open_interest_pctchange', 'volatility', 'volatility_change', 'volatility_pctchange', 'theoretical', 'delta', 'gamma', 'theta', 'vega', 'rho', 'vol_oi_ratio', 'midpoint', 'put', 'ema_12', 'ema_26', 'macd', 'macd_signal', 'intraday_range_pct', 'options_to_ohlcv_volume_ratio']\n",
    "\n",
    "# Group by each unique day so that each day’s options data is retained\n",
    "grouped = df.groupby('Date')\n",
    "days = []\n",
    "dates = []\n",
    "for date, group in grouped:\n",
    "    # Extract the feature values from the group\n",
    "    day_data = group[feature_cols].values.astype(np.float32)\n",
    "    days.append(day_data)\n",
    "    dates.append(date)\n",
    "\n",
    "# Sort the days by date\n",
    "sorted_idx = np.argsort(dates)\n",
    "days = [days[i] for i in sorted_idx]\n",
    "dates = [dates[i] for i in sorted_idx]\n",
    "\n",
    "# Create binary target values:\n",
    "# For each day, we compare its \"Close/Last\" (assumed constant for that day)\n",
    "# with the next day’s \"Close/Last\". If next day > current day, label 1; else 0.\n",
    "direction_targets = []\n",
    "for i in range(len(days) - 1):\n",
    "    current_close = days[i][0, feature_cols.index('Close/Last')]\n",
    "    next_close = days[i+1][0, feature_cols.index('Close/Last')]\n",
    "    direction = 1 if next_close > current_close else 0\n",
    "    direction_targets.append(direction)\n",
    "    \n",
    "# Remove the last day as it has no following day's target\n",
    "days = days[:-1]\n",
    "dates = dates[:-1]\n",
    "y = np.array(direction_targets)  # Binary labels (0 or 1)\n",
    "\n",
    "# Since each day may have a variable number of option records,\n",
    "# we pad the data so that every day has the same shape.\n",
    "max_options = max(day.shape[0] for day in days)\n",
    "days_padded = pad_sequences(days, maxlen=max_options, dtype='float32', \n",
    "                            padding='post', truncating='post')\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Create Lookback Sequences\n",
    "# -----------------------------------\n",
    "# Each sample is built from the previous \"lookback\" days' options data.\n",
    "lookback = 5  # For example: use the previous 5 days\n",
    "X, X_dates = [], []\n",
    "for i in range(lookback, len(y)):\n",
    "    X.append(days_padded[i-lookback:i])\n",
    "    X_dates.append(dates[i])\n",
    "X = np.array(X)  # shape: (n_samples, lookback, max_options, n_features)\n",
    "y = y[lookback:]  # Align targets with the samples\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Scale the Data (features only)\n",
    "# -----------------------------------\n",
    "# Reshape X to 2D for scaling, then reshape back.\n",
    "n_samples, lb, max_opts, n_features = X.shape\n",
    "X_reshaped = X.reshape(-1, n_features)\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled_reshaped = scaler_X.fit_transform(X_reshaped)\n",
    "X_scaled = X_scaled_reshaped.reshape(n_samples, lb, max_opts, n_features)\n",
    "# No scaling for y as they are binary.\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Build the LSTM RNN Model for Classification with L2 Regularization\n",
    "# -----------------------------------\n",
    "l2_reg = 0.001  # Regularization factor\n",
    "\n",
    "# Sub-model to process a single day’s options data.\n",
    "option_input = Input(shape=(max_options, n_features), name='option_input')\n",
    "masked = Masking(mask_value=0.0)(option_input)\n",
    "option_dense = TimeDistributed(Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)))(masked)\n",
    "day_embedding = GlobalAveragePooling1D()(option_dense)\n",
    "day_model = Model(inputs=option_input, outputs=day_embedding, name='day_model')\n",
    "\n",
    "# Sequence model: process a sequence of days.\n",
    "seq_input = Input(shape=(lookback, max_options, n_features), name='seq_input')\n",
    "day_embeddings = TimeDistributed(day_model)(seq_input)  # shape: (lookback, 64)\n",
    "lstm_out = LSTM(50, activation='tanh')(day_embeddings)\n",
    "# For classification: use a sigmoid activation.\n",
    "output = Dense(1, activation='sigmoid', kernel_regularizer=l2(l2_reg))(lstm_out)\n",
    "\n",
    "model = Model(inputs=seq_input, outputs=output, name='advanced_direction_model')\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Train the Model\n",
    "# -----------------------------------\n",
    "history = model.fit(X_scaled, y, epochs=20, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# -----------------------------------\n",
    "# 6. Evaluate and Save Predictions\n",
    "# -----------------------------------\n",
    "# Generate predictions on the entire dataset.\n",
    "predictions_probs = model.predict(X_scaled)\n",
    "# Convert probabilities to binary predictions using 0.5 threshold.\n",
    "predictions = (predictions_probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "# Create a DataFrame with the results.\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': [d.strftime('%Y-%m-%d') for d in X_dates],\n",
    "    'True_Direction': y,\n",
    "    'Predicted_Direction': predictions\n",
    "})\n",
    "print(results_df.head())\n",
    "\n",
    "# Save the trained model and predictions.\n",
    "model.save('advanced_direction_predictor.h5')\n",
    "results_df.to_csv('advanced_direction_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
